---
title: "Diabetes Classifications Models"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Chelsea Nelson"
date: today
format:
  html:
    
    toc: true
    embed-resources: true
execute:
  echo: false
  warning: false
from: markdown+emoji 
editor: 
  markdown: 
    wrap: 72
---
::: {.callout-tip icon="false" appearance="simple"}
## Github Repo Link

[Final Project II Repo](https://github.com/stat301-2-2024-winter/final-project-2-chelseanelson)
:::

```{r}
#| label: loading-packages 

library(tidyverse)
library(DT)
library(tidymodels)
library(here)
library(knitr)

```

## Introduction

The prediction objective at hand is to predict an individual's diabetes
diagnoses based on general health questions. This objective is a
binary classification problem, as the goal is to successfully identify someone as 
either having diabetes or not having diabetes.

My motivation for pursuing this particular problem is because I am
interested in seeing what factors of everyday life seem to play a role in
the diagnosing of diabetes. This information is important as it 
can help zoom into the variables that play a large part in if someone 
has diabetes or not, as well as it helps to limit the amount of people who 
do have diabetes as we can successfully understand to some extent why people
develop having diabetes. 

I will be using the Centers for Disease Control and Prevention's (CDC)
survey data from their Behavioral Risk Factor Surveillance System
(BRFSS) which is a health-related telephone survey. Although looking at
multiple aspects of life through these telephone surveys, I will specifically 
being looking at the Diabetes Health Indicators Dataset which contains
statistics and lifestyle survey information about people in general
along with their diagnosis of diabetes. More information related to this dataset
can be found in the [References](@sec-references) section, with both the official CDC website as
well as the Kaggle page which I collected the data from.

## Data Overview & Quality 

![Univariate Analysis - Diabetes Diagnosis (Original Version)](figures/figure-1.png){#fig-1}

In the original version of the dataset, there was severe class imbalance in our 
response variable of diabetes diagonsis. This can be shown in @fig-1. Additionally,
within this original version, there were three types of diagnoses that someone 
could potentially receive: no diabetes, prediabetes, or diabetes. 

In order to take care of this class imbalance however, I made an update to the 
dataset. I created my own random 50-50 split from the original larger dataset. 
Within this 50-50 split, I also changed my response variable from including three 
levels of classifications to only having two, diabetes or no diabetes. 
Within this change, I combined the pre-diabetes and diabetes levels into 
one representing now having or close to having diabetes, thus then
randomly downsampling the larger no diabetes level. I decide to combine the levels 
in this manner as according to an ADA expert panel, up to 70% of individuals 
with prediabetes will eventually develop diabetes, more information supporting 
this claim is located in the [References](@sec-references) section. 

After making these necessary changes, the response variable no longer has severe
class imbalance thus being able to have my model make better predictions, 
as class imbalance can lead to biased models that favor the majority class, thus 
performing poorly on predicting the minority class. Furthermore, this then 
accounts for many misleading interpretations of the model performance metrics
that could have arisen, allowing for me to be able to generalize the model's 
predictions better. 

![Univariate Analysis - Diabetes Diagnosis (Updated Version)](figures/figure-3.png){#fig-2}

I have visually showcased these changes through @fig-2. We can now see that 
the two levels are balanced with each consisting of 39977 instances. 

Additionally, it is important to point out that there are no instances of 
missing data within my dataset, thus I will not have to impute any variables
later on to take care of its missingness. 

Looking at further explorations of the predictor variables, I will focus on in 
this section the variables which explored bmi, high blood pressure, 
cholsterol levels, and the overall general health of the respondent. The 
rest of the EDA can be observed in [Appendix I: EDA](@sec-EDA). I chose to focus mostly on these 
variables, as after an extensive EDA, I visualized and discovered that these variables have the 
strongest relationships between themselves as well as in relationship 
with our response variable. 

### Univariate Analysis

First I will discuss the univariate distributions of each of the stated above
variables of interest. 

![BMI - Univariate Distribution](figures/univariate-EDA/numerical_univariate_1.png){#fig-3}

In @fig-3, we see that the distribution of the respondents's body mass index 
(bmi) is extremely left skewed. In order to take care of this skewness in the
actual building of my models, I decided to perform a log10 transformation on 
this predictor variable during the making of the pre-processing 
feature engineering recipe. 

![BMI - Univariate Distribution (log10)](figures/univariate-EDA/numerical_univariate_7.png){#fig-4}

@fig-4 showcases the bmi distribution of our respondents again, however now with the 
log10 transformation done to it. From this, we now see that the distribution is
more symmetric having little to no skewedness to it as well as no outliers. Thus
this confirms that by applying a log10 transformation on this variable during my 
pre-processing stage, I will hopefully be able to gain more diverse and 
substantial predictions.

![General Health - Univariate Distribution](figures/univariate-EDA/categorical_univariate_10.png){#fig-5}

Within our visualization of the general health responses (@fig-5), we see that there 
a lot of class imbalance. This is something that we will have to keep in 
mind as we go on to building the models, wanting to ensure that we recognize the
existence of this as we look at the performance metrics associated with each of
our model types. 

![Blood Pressure - Univariate Distribution](figures/univariate-EDA/categorical_univariate_12.png){#fig-6}

@fig-6 showcases that there is an almost equal balance between our two levels 
within the blood pressure variable. Thus meaning that our respondents are almost
equal in the amounts that have or don't have high blood pressure. This lack
of class imbalance is extremely good as it helps reduce biases in the predictions
that could be made by our models. 

![Cholosterol - Univariate Distribution](figures/univariate-EDA/categorical_univariate_13.png){#fig-7}

Similar to @fig-6, in @fig-7 we see that there is an almost equal balance 
between our two levels within the cholesterol variable. Thus meaning that 
from our respondents there is an almost equal divide of people who have and 
don't have high cholesterol. As stated above, this lack of severe class imbalance
will help in reducing any biases that could be obtained from our predictive 
variables when creating predictions from our models. 

### Bivariate Analysis

Turning towards looking more at the relationship between each of these 
variables, overall we see that these variables are extremely related to each other,
as well as with our response variable, diabetes diagnosis. 

![Blood Pressure by Cholostreol - Bivariate Distribution](figures/bivariate-EDA/bivariate-1.png){#fig-8}

Looking at @fig-8, there seems to be an extremely strong relationship between these 
blood pressure and cholesterol, as we see that having no high blood pressure is 
extremely more common when a respondent also does not have high cholesterol, 
and then when a respondent does have high blood pressure they are more 
likely to then also have high cholesterol. Seeing the strength and commonalities
within this relationship, I plan to include this relationship and look more at 
the interactions between these two variables within my pre-processing stage 
for building my feature engineered recipes. 

![BMI by Blood Pressure - Bivariate Distribution](figures/bivariate-EDA/bivariate-3.png){#fig-9}

When looking at the relationships displayed in @fig-9, I find it interesting that 
the overall distribution of the respondents' bmi when they don't have high blood pressure
compared to when they do is extremely similar. The only different between these
distributions lies when looking at the median, and 25th and 75th quartiles. Through 
this we see that the average bmi for people who do have high blood pressure 
is higher than that of people without high blood pressure. It will be interesting to 
see how this relationship plays a part in the predictions created by my models. 

![BMI by Cholesterol - Bivariate Distribution](figures/bivariate-EDA/bivariate-4.png){#fig-10}

Similar to @fig-9, it is interesting to see in @fig-10 that the overall 
distribution of the respondents' bmi when they don't have high cholesterol compared
to when they do is extremely similar. The only difference again can be witnessed
when looking aat the median, and 25th and 75th quartiles of the distributions. 
Thus form this information, we see that the average bmi for people who do have 
high cholesterol is higher than that of people without high cholesterol. The
degree of this relationship will also play an interesting role in the predictions
that are created by my models. 

![General Health by High Cholesterol - Bivariate Distribution](figures/bivariate-EDA/bivariate-5.png){#fig-11}

@Fig-11 expresses the relationship between the degree of general health that 
respondents felt they have and if they have high cholesterol or not. From this 
relationship we can highlight that as the degree of general health get worse,
we see that the amount of respondents who say that they also have high cholesterol
increases as well. thus showcasing that they are connected in some manner. 
Although I do not do further exploration of this relationship within my 
pre-processing stage, I think this would be an interesting relationship to look
more into their interact given the opportunity again. 


![General Health by Blood Pressure - Bivariate Distribution](figures/bivariate-EDA/bivariate-6.png){#fig-12}

Similar to the strong relationship exhibited in @fig-11, @fig-12 also express
a strong relationship between the degree of general health that respondents 
felt they have and if they have high blood pressure or not. From this relationship 
we can highlight that as the degree of general health get worse,
we see that the amount of respondents who say that they also have high blood
pressure increases as well. thus showcasing that they are connected in a 
negative manner. Although I do not do further exploration of this relationship within my 
pre-processing stage, I think this would be an interesting relationship to look
more into their interact given the opportunity again. 

![BMI by General Health - Bivariate Distribution](figures/bivariate-EDA/bivariate-7.png){#fig-13}

@fig-13 showcases that they are not completely correlated however we do see 
through the plot that as the general health assessment score get lower, 
the median rises in terms of BMI. However most of the overall
distributions look the same for each general health assessment, being left
skewed with a few outliers.  

![Blood Pressure by Diabetes - Bivariate Distribution](figures/bivariate-EDA/bivariate-10.png){#fig-14}

From @fig-14, we can see that blood pressure is extremely connected with 
diabetes diagnosis, as we see the trends of where someone does not have high
blood pressure, they also do not have diabetes, whereas if someone does have
high blood pressure, they tend to have diabetes versus not. Because of how 
correlated this relationship seems I will take precautions of this when
doing my pre-processing work, such that it will help mitigate the power of this 
variable in the predictive model, both in terms of its relationship to the 
response variable but other predictor variables as well. 

![Cholesterol by Diabetes - Bivariate Distribution](figures/bivariate-EDA/bivariate-11.png){#fig-15}

The visualization of @fig-15 showcases to us the strong relationship that is 
present between cholesterol levels and diabetes diagnosis. Similar to the 
relationship showcased in @fig-14, we see the trend of not having high 
cholesterol being associated with not having diabetes, while having high
cholesterol is more associated with having diabetes. Thus, I will also work
to mitigate this relationship as well as high correlations present with other
predictor variables. 

![General Health by Diabetes - Bivariate Distribution](figures/bivariate-EDA/bivariate-12.png){#fig-16}

@fig-16 highlights the strong relationship that seems to be present between
respondents' general health assessment score and their diabetes diagnosis. 
We see that as the assessment scores get worse, people are more and more 
likely to have diabetes than not. As with the blood pressure and cholesterol 
variables, I will have to work around this strong relationship within my 
pre-processing stage, in order to mitigate any additional biases it could 
create for my predictions.

![BMI by Diabetes - Bivariate Distribution](figures/bivariate-EDA/bivariate-13.png){#fig-17}

Lastly from @fig-17, we are able to look at the relationship between body
mass index and diabetes diagnosis. There seems to be slightly strong 
relationship between the two of them, as we see that the median and two quartiles 
body mass indexes associated with each distribution increases when the person
has diabetes versus when they do. 

After looking at these variables as well as others that will be expressed in 
the [Appendix I: EDA](@sec-EDA), I am really to add on more aspects to 
my pre-processing stage for my second versions of my models 
than what I did previously when working with my kitchen
sink and baseline models. 

## Methods {#sec-methods}

In order to conduct and build my predictive models, I decided to 
split the dataset into training and testing sets using a initial 75-25 split. 
This is because I felt that since there are a lot of instances in my 
dataset, I could put more of that data into the testing set then I 
normally do with a regular 80-20 split. Thus this allowed for more 
instances to be available for evaluating the performance of the final model, 
potentially resulting in more reliable estimates of model performance and 
better generalization to unseen data.

After my initial split, I decided to conduct a cross-validation resampling 
technique employing 10 folds with 5 repeats to robustly estimate the models'
performance and account for potential variability in the data. By repeating 
the cross-validation process 5 times, I am able to obtain more reliable 
estimates of model performance. Additionally, with 10-fold cross-validation, 
the training data is divided into 10 roughly equal-sized folds. 
Each fold serves as both a training and testing set, 
allowing me to utilize a larger portion of the data for training and 
testing purposes. This maximizes the use of available data and 
provides more accurate estimates of model performance. Each
model type will be fitted 50 times, allowing for good predictions to be made
in terms of finding the best hyperparameters for my final model. 

Additionally, as stated above it is important acknowledge that I will be conducting 
a binary classification prediction problem, with the primary assessment metric 
for the analysis will be accuracy. Accuracy provides a clear measure of the 
overall correctness of the model's predictions, indicating the proportion of 
correctly classified instances out of the total number of instances. 

In terms of the model types that I will be fitting, I have selected to use 
the Naive Bayes, Logistic Regression, Random Forest, Boosted Tree, 
$k$-Nearest Neighbors, and a Single-Layer Neural Network models for 
my model analysis. I have decided to use these models as they all provide me
with an arrangement of models from baseline to highly advanced models to see
what factors play into discovering diabetes diagnosis in the best manner. 

For each model type, I have taken to tuning different parameters in order
to see the different results that could be generated wanting to ultimate 
pick the best model possible. In terms of iteration, I used a regular grid 
for the tuning process of all of the models, thus where all of the combinations 
of the hyperparameters within the specified ranges will be evaluated exhaustively.
In terms which parameters to tune and to how much, I based my decisions at least
for the kitchen sink models on past examples done in class, as well as relying 
on the originally given ranges as well. 

(look over and rewrite some parts of this section potentially)
Naive Bayes: I did not tune any of the parameters for this model as I was 
using it as my baseline model thus I felt no need to execute or change the 
given parameters, thus rather just fitting the base values of it to my 
resamples. 

Logistic Regression: There were no hyperparameters for this model, thus I 
did not need to tune anything when fitting the kitchen sink version or the feature
engineering version of the model to my resamples. 

Random Forest: For my kitchen sink random forest model, I decided to tune the 
`trees`, `min_n`, and `mtry` parameters. The `trees` parameter represents the 
number of trees to be grown in the random forest. For this hyperparameter I 
decided to look at a range from 100 to 2000, with increments of 6. Thus I will 
be looking at around 317 different values for this parameter, with a grid size 
of 6 levels. The `min_n` parameter represents the minimum number of samples 
required to split a node in the tree. I decided for this parameter to go with 
the range that they already previous associated with this parameter. However,
I decide to look at the range with increments of 4. Thus I will be looking at 
around ___ different values for this parameter, with a grid size of 4 levels. 
Lastly, I decided to tune the `mtry` parameters which represents the number of 
variables randomly sampled as candidates at each split. I decided to look at 
a range from 1 to 7, with increments of 1. Thus I will be looking at 7 different
values, with a grid size of 1 level. However for my feature engineering random
forest model, I decided to base what I tune on how well my kitchen sink random
forest model performed beforehand. Thus for this model, I tuned the ____ 
parameters. (explain why I did each of these)

Boosted Trees:

$K$-Nearest Neighbors: For my kitchen sink $k$-nearest neighbors model, I 
decided to tune the `neighbors` parameter. The `neighbors` parameter represents 
____ . For this hyperparameter, I decided to look at the range from 1 to 15, 
with increments of 5. Thus I will be looking at around 3 different values 
for this parameter, with a grid size of 5 levels. However for my feature
engineering $k$-nearest neighbors model, I decided to stick with the original
range given which is 1 to 10 with increments of 5. 
Thus I will be looking at around 2 values for this parameter, with a
grid size of 5 levels. (this doesn't seem right). I chose to make this change 
because _____. 

Neutral Network: For my kitchen sink neutral network model, I decided to tune the
`epochs` and `hidden_units` parameters. The `epochs` parameter represents ____. 
For this hyperparameter, I decided to stick with the original range given which
is from 10 to 1000, with increments of 5. Thus I will be looking at around 
199 different values for this parameter, with a grid size of 5 levels. 
The `hidden_units` parameter represents ____. I decided for this parameter 
to also stick with the original range given which is from 1 to 10, with 
increments of 5. Therefore, I will be looking at around 2 different values for 
this parameter, with a grid size of 5 levels. For the feature engineering 
neutral network model, I decided to stick with a similar tuning and resampling
method, however changing the amount of levels from 5 to 4 to ensure a faster 
model tuning sequence. 

- Describe any parameters that will be tuned
    - What are the ranges to be used?
    - How many different values are you looking at?
    - What is the grid size of the parameter?
    - What type of iterations are you using?
 
In total, I made 5 different recipes to be used with my models. Three of those
models are baseline/kitchen sink recipes, whereas the last two have more 
feature engineering components. 

Looking first at the kitchen sink recipes, I first made one specifically for 
my baseline Naive Bayes model. Within this recipe, I included _____. 
I did this because ______. I did not dummy or turn numerical my categorical 
variables for this recipe, as I will do for the rest,
as the Naive Bayes model is able to work around being given straight categorical
variables, within no need for change. The second kitchen sink recipe that I created
was for my parametric models, neutral network and logistic regression models. 
For this recipe, I included _______. I did this because ______. As this was 
the kitchen sink/baseline recipe for my models, I decided not to add multiple 
different factors into its creation. Lastly in terms of my kitchen sink recipes,
I made one specifically for my tree-based models thus the boosted trees, random
forest, and $k$-nearest neighbors models. This recipe includes ______. I 
did this because ______. Unlike the parametric models recipe, I decided to one 
hot code my categorical variables because ______. 

Turning the attention to my feature engineering recipes, one was designed 
to again fit my parametric models, being the logistic regression and neutral 
network models. Whereas the other was fit to work with the tree-based models, 
being the boosted trees, random forest, and $k$-nearest neighbors models. 
When observing the former recipe, I now included ______ in addition to the 
aspects that were already apart of the kitchen sink parametric recipe. I decided
to add these specific aspects as I felt they would add more complex and definition 
to the building of my models. Within the latter recipe, I decided to add ______
in addition to the aspects that were already apart of the kitchen sink tree-based
recipe. I found these additions necessary as I felt they targeted points of interest 
or lack there of, in terms of helping produce the best models for predicting 
diabetes diagnosis. Unlike in the feature engineering parametric recipe, 
I did not include any interacts between the different predictor variables as 
the tree-based models already cover for any interacts between variables that
could also be affecting the model's performance. 

## Model Building 

As stated in the [Methods](@sec-methods) sections, the primary assessment metric 
for the analysis will be accuracy, helping to compare models and 
determine which will be the final/winning model. Accuracy provides a clear 
measure of the overall correctness of the model's predictions, indicating the 
proportion of correctly classified instances out of the total number of instances. 

**Performance Comparison Table**
```{r}
#| label: performance_model 

read_rds(here("results/tuned_models/model_accuracy_comparison.rds")) %>% knitr::kable()
```

After looking at the best tuned hyperparameters for each of the model types, 
I think it would be beneficial in the future to examine and try more combinations
to hopefully find and achieve even better predictions and accuracy metrics. In
terms of the different model types, I have listed out what were concluded to be
the best parameters for each.

Neutral Network (kitchen sink): `epochs` = 257 and `hidden_units` = 7

Neutral Network (feature engineering): `epochs` = 1000 and `hidden_units` = 4

$K$-Nearest Neighbors (kitchen sink): `neighbors` = 15

$K$-Nearest Neighbors (feature engineering): `neighbors` = 15

Random Forest (kitchen sink): 

Random Forest (feature engineering):

Boosted Tree (kitchen sink):

Boosted Tree (feature engineering):

Additionally, it is interesting to see that after adding on more steps and 
features in the recipes, the performance of the logistics model type increased, 
whereas the performance of the neutral network model type got worse. The 
$k$-nearest neighbors model type did not really change, although gaining more 
room for standard error in overall how the tuned resampling models performed. It would 
be interesting to go back in the future and create another recipe to see where
the actual performance limiting elements are located within the recipe. 

After removing the variables assessing if someone had healthcare, if they 
needed to see a doctor, but didn't have money or not, and  if someone had had 
a cholesterol check in the last five years or not from being predictive variables 
in my pre-processing feature engineered recipes, it is interesting to 
see that the performance of said models did not follow my hypothesis and 
thoughts on how those variables do not affect the response variable much in 
terms of having predictive importance, as the feature engineered models did not
see much change in the accuracy of their predictions. 

Overall, as seen in the above performance comparison table, I will be 
selecting the _____ ______ model as the final model to test as it has the 
best performance when being assessed via our main performance metric of 
accuracy. Additionally, because of the small standard deviation associated 
with the values, we can conclude that on average, most of the different tuned 
models stayed around this level of accuracy. It was surprising or not 
surprising that this model won has ____. 

## Final Model Analysis

- Where you fit your final/winning model to the full training and then testing data
- Assess the final model’s performance with at least the metric used to determine the winning model, could also use other metrics as well 
- Should include an exploration of predictions vs the true values (confusion matrix)
- Is the model any good? IT might be the best of the models you tried, but does the effort of building a predictive model really pay off – is it that much better than a baseline/null model?
- Were there any features of the model you selected that make it the best (e.g. fits nonlinearity well)?

## Conclusions 

- State any conclusions or discoveries/insights. 
- This is a great place for future work, new research questions, and next steps 

## References {#sec-references}

  1. Kaggle Diabetes Health Indicators Dataset [https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/data?select=diabetes_012_health_indicators_BRFSS2015.csv](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/data?select=diabetes_012_health_indicators_BRFSS2015.csv)

  2. CDC Behavioral Risk Factor Surveillance System [https://www.cdc.gov/brfss/index.html](https://www.cdc.gov/brfss/index.html)

  3. National Library of Medicine - Prediabetes: A high-risk state for developing diabetes 
  [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3891203/#:~:text=According%20to%20an%20ADA%20expert,prediabetes%20will%20eventually%20develop%20diabetes.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3891203/#:~:text=According%20to%20an%20ADA%20expert,prediabetes%20will%20eventually%20develop%20diabetes.) 

## Appendix 

### Appendix I: EDA {#sec-EDA}

### Appendix II: Extras {#sec-extras}

