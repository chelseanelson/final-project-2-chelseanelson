---
title: "Diabetes Classifications Models"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Chelsea Nelson"
date: today
format:
  html:
    
    toc: true
    embed-resources: true
execute:
  echo: false
  warning: false
from: markdown+emoji 
editor: 
  markdown: 
    wrap: 72
---
::: {.callout-tip icon="false" appearance="simple"}
## Github Repo Link

[Final Project II Repo](https://github.com/stat301-2-2024-winter/final-project-2-chelseanelson)
:::

```{r}
#| label: loading-packages 

library(tidyverse)
library(DT)
library(tidymodels)
library(here)
library(knitr)

```

## Introduction

The prediction objective at hand is to predict an individual's diabetes
diagnoses based on general health questions. This objective is a
binary classification problem, as the goal is to successfully identify someone as 
either having diabetes or not having diabetes.

My motivation for pursuing this particular problem is because I am
interested in seeing what factors of everyday life seem to play a role in
the diagnosing of diabetes. This information is important as it 
can help zoom into the variables that play a large part in if someone 
has diabetes or not, as well as it helps to limit the amount of people who 
do have diabetes as we can successfully understand to some extent why people
develop having diabetes. 

I will be using the Centers for Disease Control and Prevention's (CDC)
survey data from their Behavioral Risk Factor Surveillance System
(BRFSS) which is a health-related telephone survey. Although looking at
multiple aspects of life through these telephone surveys, I will specifically 
being looking at the Diabetes Health Indicators Dataset which contains
statistics and lifestyle survey information about people in general
along with their diagnosis of diabetes. More information related to this dataset
can be found in the [References](@sec-references) section, with both the official CDC website as
well as the Kaggle page which I collected the data from.

## Data Overview & Quality {#sec-data}

![Diabetes Diagnosis (Original Version) - Univariate Distribution](figures/figure-1.png){#fig-1}

In the original version of the dataset, there was severe class imbalance in our 
response variable of diabetes diagnosis. This can be shown in @fig-1. Additionally,
within this original version, there were three types of diagnoses that someone 
could potentially receive: no diabetes, prediabetes, or diabetes. 

In order to take care of this class imbalance however, I made an update to the 
dataset. I created my own random 50-50 split from the original larger dataset. 
Within this 50-50 split, I also changed my response variable from including three 
levels of classifications to only having two, diabetes or no diabetes. 
Within this change, I combined the pre-diabetes and diabetes levels into 
one representing now having or close to having diabetes, thus then
randomly downsampling the larger no diabetes level. I decide to combine the levels 
in this manner as according to an ADA expert panel, up to 70% of individuals 
with prediabetes will eventually develop diabetes, more information supporting 
this claim is located in the [References](@sec-references) section. 

After making these necessary changes, the response variable no longer has severe
class imbalance thus being able to have my model make better predictions, 
as class imbalance can lead to biased models that favor the majority class, thus 
performing poorly on predicting the minority class. Furthermore, this then 
accounts for many misleading interpretations of the model performance metrics
that could have arisen, allowing for me to be able to generalize the model's 
predictions better. 

![Diabetes Diagnosis (Updated Version) - Univariate Distribution](figures/figure-3.png){#fig-2}

I have visually showcased these changes through @fig-2. We can now see that 
the two levels are balanced with each consisting of 39977 instances. 

Additionally, it is important to point out that there are no instances of 
missing data within my dataset, thus I will not have to impute any variables
later on to take care of its missingness. 

Looking at further explorations of the predictor variables, I will focus on in 
this section the variables which explored bmi, high blood pressure, 
cholesterol levels, and the overall general health of the respondent. The 
rest of the EDA can be observed in [Appendix I: EDA](@sec-EDA). I chose to focus mostly on these 
variables, as after an extensive EDA, I visualized and discovered that these variables have the 
strongest relationships between themselves as well as in relationship 
with our response variable. 

### Univariate Analysis

First I will discuss the univariate distributions of each of the stated above
variables of interest. 

![BMI - Univariate Distribution](figures/univariate-EDA/numerical_univariate_1.png){#fig-3}

In @fig-3, we see that the distribution of the respondent's body mass index 
(bmi) is extremely left skewed. In order to take care of this skewness in the
actual building of my models, I decided to perform a log10 transformation on 
this predictor variable during the making of the pre-processing 
feature engineering recipe. 

![BMI - Univariate Distribution (log10)](figures/univariate-EDA/numerical_univariate_7.png){#fig-4}

@fig-4 showcases the bmi distribution of our respondents again, however now with the 
log10 transformation done to it. From this, we now see that the distribution is
more symmetric having little to no skewedness to it as well as no outliers. Thus
this confirms that by applying a log10 transformation on this variable during my 
pre-processing stage, I will hopefully be able to gain more diverse and 
substantial predictions.

![General Health - Univariate Distribution](figures/univariate-EDA/categorical_univariate_10.png){#fig-5}

Within our visualization of the general health responses (@fig-5), we see that there 
a lot of class imbalance. This is something that we will have to keep in 
mind as we go on to building the models, wanting to ensure that we recognize the
existence of this as we look at the performance metrics associated with each of
our model types. 

![Blood Pressure - Univariate Distribution](figures/univariate-EDA/categorical_univariate_12.png){#fig-6}

@fig-6 showcases that there is an almost equal balance between our two levels 
within the blood pressure variable. Thus meaning that our respondents are almost
equal in the amounts that have or don't have high blood pressure. This lack
of class imbalance is extremely good as it helps reduce biases in the predictions
that could be made by our models. 

![Cholosterol - Univariate Distribution](figures/univariate-EDA/categorical_univariate_13.png){#fig-7}

Similar to @fig-6, in @fig-7 we see that there is an almost equal balance 
between our two levels within the cholesterol variable. Thus meaning that 
from our respondents there is an almost equal divide of people who have and 
don't have high cholesterol. As stated above, this lack of severe class imbalance
will help in reducing any biases that could be obtained from our predictive 
variables when creating predictions from our models. 

### Bivariate Analysis

Turning towards looking more at the relationship between each of these 
variables, overall we see that these variables are extremely related to each other,
as well as with our response variable, diabetes diagnosis. 

![Blood Pressure by Cholostreol - Bivariate Distribution](figures/bivariate-EDA/bivariate-1.png){#fig-8}

Looking at @fig-8, there seems to be an extremely strong relationship between these 
blood pressure and cholesterol, as we see that having no high blood pressure is 
extremely more common when a respondent also does not have high cholesterol, 
and then when a respondent does have high blood pressure they are more 
likely to then also have high cholesterol. Seeing the strength and commonalities
within this relationship, I plan to include this relationship and look more at 
the interactions between these two variables within my pre-processing stage 
for building my feature engineered recipes. 

![BMI by Blood Pressure - Bivariate Distribution](figures/bivariate-EDA/bivariate-3.png){#fig-9}

When looking at the relationships displayed in @fig-9, I find it interesting that 
the overall distribution of the respondents' bmi when they don't have high blood pressure
compared to when they do is extremely similar. The only different between these
distributions lies when looking at the median, and 25th and 75th quartiles. Through 
this we see that the average bmi for people who do have high blood pressure 
is higher than that of people without high blood pressure. It will be interesting to 
see how this relationship plays a part in the predictions created by my models. 

![BMI by Cholesterol - Bivariate Distribution](figures/bivariate-EDA/bivariate-4.png){#fig-10}

Similar to @fig-9, it is interesting to see in @fig-10 that the overall 
distribution of the respondents' bmi when they don't have high cholesterol compared
to when they do is extremely similar. The only difference again can be witnessed
when looking at the median, and 25th and 75th quartiles of the distributions. 
Thus form this information, we see that the average bmi for people who do have 
high cholesterol is higher than that of people without high cholesterol. The
degree of this relationship will also play an interesting role in the predictions
that are created by my models. 

![General Health by High Cholesterol - Bivariate Distribution](figures/bivariate-EDA/bivariate-5.png){#fig-11}

@Fig-11 expresses the relationship between the degree of general health that 
respondents felt they have and if they have high cholesterol or not. From this 
relationship we can highlight that as the degree of general health get worse,
we see that the amount of respondents who say that they also have high cholesterol
increases as well. thus showcasing that they are connected in some manner. 
Although I do not do further exploration of this relationship within my 
pre-processing stage, I think this would be an interesting relationship to look
more into their interact given the opportunity again. 


![General Health by Blood Pressure - Bivariate Distribution](figures/bivariate-EDA/bivariate-6.png){#fig-12}

Similar to the strong relationship exhibited in @fig-11, @fig-12 also express
a strong relationship between the degree of general health that respondents 
felt they have and if they have high blood pressure or not. From this relationship 
we can highlight that as the degree of general health get worse,
we see that the amount of respondents who say that they also have high blood
pressure increases as well. thus showcasing that they are connected in a 
negative manner. Although I do not do further exploration of this relationship within my 
pre-processing stage, I think this would be an interesting relationship to look
more into their interact given the opportunity again. 

![BMI by General Health - Bivariate Distribution](figures/bivariate-EDA/bivariate-7.png){#fig-13}

@fig-13 showcases that they are not completely correlated however we do see 
through the plot that as the general health assessment score get lower, 
the median rises in terms of BMI. However most of the overall
distributions look the same for each general health assessment, being left
skewed with a few outliers.  

![Blood Pressure by Diabetes - Bivariate Distribution](figures/bivariate-EDA/bivariate-10.png){#fig-14}

From @fig-14, we can see that blood pressure is extremely connected with 
diabetes diagnosis, as we see the trends of where someone does not have high
blood pressure, they also do not have diabetes, whereas if someone does have
high blood pressure, they tend to have diabetes versus not. Because of how 
correlated this relationship seems I will take precautions of this when
doing my pre-processing work, such that it will help mitigate the power of this 
variable in the predictive model, both in terms of its relationship to the 
response variable but other predictor variables as well. 

![Cholesterol by Diabetes - Bivariate Distribution](figures/bivariate-EDA/bivariate-11.png){#fig-15}

The visualization of @fig-15 showcases to us the strong relationship that is 
present between cholesterol levels and diabetes diagnosis. Similar to the 
relationship showcased in @fig-14, we see the trend of not having high 
cholesterol being associated with not having diabetes, while having high
cholesterol is more associated with having diabetes. Thus, I will also work
to mitigate this relationship as well as high correlations present with other
predictor variables. 

![General Health by Diabetes - Bivariate Distribution](figures/bivariate-EDA/bivariate-12.png){#fig-16}

@fig-16 highlights the strong relationship that seems to be present between
respondents' general health assessment score and their diabetes diagnosis. 
We see that as the assessment scores get worse, people are more and more 
likely to have diabetes than not. As with the blood pressure and cholesterol 
variables, I will have to work around this strong relationship within my 
pre-processing stage, in order to mitigate any additional biases it could 
create for my predictions.

![BMI by Diabetes - Bivariate Distribution](figures/bivariate-EDA/bivariate-13.png){#fig-17}

Lastly from @fig-17, we are able to look at the relationship between body
mass index and diabetes diagnosis. There seems to be slightly strong 
relationship between the two of them, as we see that the median and two quartiles 
body mass indexes associated with each distribution increases when the person
has diabetes versus when they do. 

After looking at these variables as well as others that will be expressed in 
the [Appendix I: EDA](@sec-EDA), I am ready to add on more aspects to 
my pre-processing stage for my second versions of my models 
than what I did previously when working with my kitchen
sink and baseline models. 

## Methods {#sec-methods}

In order to conduct and build my predictive models, I decided to 
split the dataset into training and testing sets using a initial 75-25 split. 
This is because I felt that since there are a lot of instances in my 
dataset, I could put more of that data into the testing set then I 
normally do with a regular 80-20 split. Thus this allowed for more 
instances to be available for evaluating the performance of the final model, 
potentially resulting in more reliable estimates of model performance and 
better generalization to unseen data.

After my initial split, I decided to conduct a cross-validation resampling 
technique employing 10 folds with 5 repeats to robustly estimate the models'
performance and account for potential variability in the data. By repeating 
the cross-validation process 5 times, I am able to obtain more reliable 
estimates of model performance. Additionally, with 10-fold cross-validation, 
the training data is divided into 10 roughly equal-sized folds. 
Each fold serves as both a training and testing set, 
allowing me to utilize a larger portion of the data for training and 
testing purposes. This maximizes the use of available data and 
provides more accurate estimates of model performance. Each
model type will be fitted 50 times, allowing for good predictions to be made
in terms of finding the best hyperparameters for my final model. 

Additionally, as stated above it is important acknowledge that I will be conducting 
a binary classification prediction problem, with the primary assessment metric 
for the analysis will be accuracy. Accuracy provides a clear measure of the 
overall correctness of the model's predictions, indicating the proportion of 
correctly classified instances out of the total number of instances. 

In terms of the model types that I will be fitting, I have selected to use 
the Naive Bayes, Logistic Regression, Random Forest, Boosted Tree, 
$k$-Nearest Neighbors, and a Single-Layer Neural Network models for 
my model analysis. I have decided to use these models as they all provide me
with an arrangement of models from baseline to highly advanced models to see
what factors play into discovering diabetes diagnosis in the best manner. 

For each model type, I have taken to tuning different parameters in order
to see the different results that could be generated wanting to ultimate 
pick the best model possible. In terms of iteration, I used a regular grid 
for the tuning process of all of the models, thus where all of the combinations 
of the hyperparameters within the specified ranges will be evaluated exhaustively.
In terms which parameters to tune and to how much, I based my decisions at least
for the kitchen sink models on past examples done in class, as well as relying 
on the originally given ranges as well. Afterwards, my goal was to assign my 
tuning decisions for the feature-engineered models by looking at the best
tuning parameters from the kitchen sink models. However, within this decision
as well I had to weigh my the cost of tuning to a heavy extent with being time
efficient in building the models. I have included specifics on how I tuned
each model, within [Appendix II: Tuning Models Selections](#sec-tuning) 
 
In total, I made 5 different recipes to be used with my models. Three of those
models are baseline/kitchen sink recipes, whereas the last two have more 
feature engineering components. 

Looking first at the kitchen sink recipes, I first made one specifically for 
my baseline Naive Bayes model. Within this recipe, I included the removal of zero variance, 
as well as normalized my numerical predictors. I did these features help 
reduce the model's complexity and potentially avoid issues such as overfitting. 
These preprocessing steps help create a cleaner, more informative dataset for
training the model, potentially leading to better generalization and predictive 
accuracy on unseen data. I did not dummy or turn numerical my categorical 
variables for this recipe, as the Naive Bayes model is able to work around 
being given straight categorical variables, with no need for change. 

The second kitchen sink recipe that I created was for my parametric models, 
the neural network and logistic regression models. 
For this recipe, I included similar features such as the removal of zero variance,
and the normalization of my predictors for similar reasons as mention before. 
Within this version, I also now included the altering of the categorical 
variables to being numerical through using treatment encoding, being able to
to improve the performance of your models by enhancing their ability to 
interpret and utilize categorical information. As this was the kitchen 
sink/baseline recipe for my models, I decided not to add multiple different 
factors into its creation.

Lastly in terms of my kitchen sink recipes, I made one specifically for my 
tree-based models thus the boosted trees, random forest, and $k$-nearest 
neighbors models. This recipe includes the same features that were seen
in the kitchen sink parameteric models recipe, however instead of doing 
treatment encoding, I did one-hot encoding when transforming my categorical
variables to being numeric. I did this because type of encoding is 
very versatile works well when when there is no natural order among the 
categories.

Turning the attention to my feature engineering recipes, one was designed 
to again fit my parametric models, being the logistic regression and neural 
network models. Whereas the other was fit to work with the tree-based models, 
being the boosted trees, random forest, and $k$-nearest neighbors models. 

When observing the former recipe, I now included the removal of variables, that 
I felt did not contribute to the prediction of diabetes diagnosis, those 
being if the respondent had healthcare or not, if they had had a cholesterol check
in the past five years, and if they needed to go to the doctor but couldn't pay 
for it. Additionally, I included the a log transformation for some of my numerical
variables, as when looking at their univariate distributions, they showcased
a lot of skewedness. Furthermore, I wanted to control for the correlated between
certain variables, thus including a feature to lessen their influence if 
two variables correlations was greater than 0.6. Lastly, I had two interacts features
between my blood pressure and cholesterol variables, as well as one between 
my bmi and general health assessment variables. I did this because, these specific
variables as highlighted in the [Data Overview & Quality](#sec-data) section seem
to have extremely strong relationships. All of these changes are in addition to the 
aspects that were already apart of the kitchen sink parametric recipe. I decided
to add these specific aspects as I felt they would add more complex and definition 
to the building of my models. 

Within the latter recipe, I decided to add all of the new features mentioned
in my featured engineered parameteric recipe, although not including 
the interaction features as tree-based models already cover for any 
interacts between variables that could also be affecting the model's performance. Again, 
these additions are in addition to the aspects that were already apart 
of the kitchen sink tree-based recipe. I found these additions necessary as 
I felt they targeted points of interest or lack there of, in terms of helping 
produce the best models for predicting diabetes diagnosis.

## Model Building 

As stated in the [Methods](@sec-methods) sections, the primary assessment metric 
for the analysis will be accuracy, helping to compare models and 
determine which will be the final/winning model. Accuracy provides a clear 
measure of the overall correctness of the model's predictions, indicating the 
proportion of correctly classified instances out of the total number of instances. 

**Performance Comparison Table**
```{r}
#| label: performance_model 

read_rds(here("results/tuned_models/model_accuracy_comparison.rds")) %>% knitr::kable()
```

After looking at the best tuned hyperparameters for each of the model types, 
I think it would be beneficial in the future to examine and try more combinations
to hopefully find and achieve even better predictions and accuracy metrics. In
terms of the different model types, I have listed out what were concluded to be
the best parameters for each.

Neural Network (kitchen sink): `epochs` = 257 and `hidden_units` = 7

Neural Network (feature engineering): `epochs` = 1000 and `hidden_units` = 4

$K$-Nearest Neighbors (kitchen sink): `neighbors` = 15

$K$-Nearest Neighbors (feature engineering): `neighbors` = 15

Random Forest (kitchen sink): `mtry` = 5, `trees` = 2000, and `min_n` = 40

Random Forest (feature engineering): `mtry` = 5 and `min_n` = 40

Boosted Tree (kitchen sink): `mtry` = 5, `trees` = 1240, `min_n` = 27, and `learn_rate` = 0.0158

Boosted Tree (feature engineering): `mtry` = 5, `min_n` = 40, `learn_rate` = 0.631

Additionally, it is interesting to see that after adding on more steps and 
features in the recipes, the performance of the logistics model type increased, 
whereas the performance of the neural network and boosted tree model types got worse. The 
$k$-nearest neighbors and random forest model types did not really change, although gaining more 
room for standard error in overall how the tuned resampling models performed. It would 
be interesting to go back in the future and create another recipe to see where
the actual performance limiting elements are located within the recipe. As
well as tuning the parameters in different ranges to look and see different
results. I think for example, that because I decided to tune less parameters
in most of my heavily feature engineered models, this impacted their performance
results. 

After removing the variables assessing if someone had healthcare, if they 
needed to see a doctor, but didn't have money or not, and  if someone had had 
a cholesterol check in the last five years or not from being predictive variables 
in my pre-processing feature engineered recipes, it is interesting to 
see that the performance of said models did not really change in performance. Thus,
meaning that these variables did not really have a presence within my predictive
models, however at the same time not adding biases to the models as well
when they were included. 

Overall, as seen in the above performance comparison table, I will be 
selecting the kitchen sink boosted tree model as the final model to test as it has the 
best performance when being assessed via our main performance metric of 
accuracy. Additionally, because of the small standard deviation associated 
with the values, we can conclude that on average, most of the different tuned 
models stayed around this level of accuracy. It was
surprising that my kitchen sink boosted tree model performed the best has I felt 
that there were still some biases that played into this model, that I felt I 
controlled better for in my heavily feature engineered models. However, I felt more surprising on 
the overall rankings of the models, as I did not expect for my featured engineered
logistic model to performance better than my featured engineered boosted tree,
$k$-nearest neighbors models, and random forest models. Also I did not expect for 
both of my neural network models to perform so well in comparison to the others. 
This is because I have never used them before, so I was unaware going in on 
how they performed usually. I feel it would be interesting to go back in the 
future and look at these differences in more depth. 

## Final Model Analysis

After fitting my winning model to the full training and then testing data, I can 
now assess the final model's performance being able to see how well the model 
generalizes to unseen data by examining its performance metrics on the testing 
dataset. Thus providing an indication of how well the model is likely to 
perform in real-world scenarios.

```{r}
#| label: performance-tables

read_rds(here("results/final_model/performance_table.rds")) %>% knitr::kable()
```

**Accuracy**: This metric measures the proportion of instances in which the predicted
value was correctly classified. Thus, being the ratio of correctly predicted 
instances to the total instances. Thus the value of 0.75 tells us that 
the model had an accuracy of 75% in terms of correctly classifying if someone
had diabetes or not. 
 

**MCC**: MCC is a metric that takes into account all four elements of the 
confusion matrix (true positives, true negatives, false positives, and 
false negatives). It ranges from -1 to 1, where 1 indicates perfect prediction, 
0 indicates random prediction, and -1 indicates total disagreement between 
prediction and observation. An MCC value of approximately 0.496 for 
indicates a moderate level of correlation between the predicted and actual 
instances.

![Confusion Matrix Heatmap](figures/final_model/heatmap.png){#heatmap}

The [Confusion Matrix](@heatmap) tells us how the boosted tree model's predictions aligned
with the truth from the testing set. Thus from the matrix, we can see that 
the model currently predicted 7926 instances where the people did have 
diabetes, being a true positive, however having 2069 instances where it incorrectly 
predicted someone had diabetes when they didn't, being a false positive. From
the other side, we see that the model correctly predicted 7005 instances where
the people did not have diabetes, being a true negative, however there being 2990
instances of error, as the model predicted that they did not have diabetes when 
they actually did, creating a false negative. This matrix seems to align well
with our accuracy value of 0.75.

I think the final model is extremely good, as the accuracy value did not 
change terribly from the resampling training data to the test data. Additionally,
this model perform a lot better than the baseline naive bayes model, which 
had an accuracy value of 68% in comparison to the 75% accuracy rate we 
achieved with our final model. I think in the future it would be beneficial 
however to go back and try out different pre-processing techniques to see
where the true difference can be made in terms of increasing our accuracy
value even more. I think that the boosted tree models reliability in terms of 
its scalability and flexibility in the type of data it receives helps to make it 
the best model out of the ones that I chose to use. 

## Conclusions 

In conclusion, we were able to approach our goal of predicting diabetes 
diagnoses based on general health questions, through utilizing various machine 
learning models and extensive preprocessing techniques. 

Within it is important to highlight again, the initial class imbalances in our 
dataset, which was addressed by creating a new balanced dataset with simplified 
response variables. After preprocessing, which included feature engineering and 
addressing skewed distributions, our models were trained and evaluated.

The final model, a boosted tree model, demonstrated strong performance with an 
accuracy of 75% and a moderate Matthews Correlation Coefficient (MCC) of 
approximately 0.496 on the testing dataset. 

The confusion matrix heatmap helps to illustrated the model's predictions,
aligning well with the actual values, reflecting the model's effectiveness in 
correctly classifying instances of diabetes diagnoses.

Overall, the boosted tree model stood out among others, showcasing scalability, 
flexibility, and reliable performance. Despite some surprises in model performance 
rankings, the final model's accuracy surpassed that of baseline models, 
indicating its potential in real-world applications. 

In the future I would like to go back and look more in depth at which tuning
parameters worked and provided the best results for each model, thus being
able to dial more into what works and what doesn't. As I felt that did not use 
the most reliable methods in terms of how I picked my tuning parameters, rather I focused
more on time efficiency.

## References {#sec-references}

  1. Kaggle Diabetes Health Indicators Dataset [https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/data?select=diabetes_012_health_indicators_BRFSS2015.csv](https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset/data?select=diabetes_012_health_indicators_BRFSS2015.csv)

  2. CDC Behavioral Risk Factor Surveillance System [https://www.cdc.gov/brfss/index.html](https://www.cdc.gov/brfss/index.html)

  3. National Library of Medicine - Prediabetes: A high-risk state for developing diabetes 
  [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3891203/#:~:text=According%20to%20an%20ADA%20expert,prediabetes%20will%20eventually%20develop%20diabetes.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3891203/#:~:text=According%20to%20an%20ADA%20expert,prediabetes%20will%20eventually%20develop%20diabetes.) 

## Appendix 

### Appendix I: EDA {#sec-EDA}

Within this section I will provide the distributions of all of the other
predictor variables. As well as any further bivariate distributions in which 
I looked at but did not express in the [Data Overview & Quality](@sec-data) section. 

#### Univariate Analysis

![Cholesterol Check - Univariate Distribution](figures/univariate-EDA/categorical_univariate_1.png)

![Stroke - Univariate Distribution](figures/univariate-EDA/categorical_univariate_2.png)

![Heart Diseaseor Attack - Univariate Distribution](figures/univariate-EDA/categorical_univariate_3.png)

![Phyisical Activity - Univariate Distribution](figures/univariate-EDA/categorical_univariate_4.png)

![Fruits - Univariate Distribution](figures/univariate-EDA/categorical_univariate_5.png)

![Vegetables - Univariate Distribution](figures/univariate-EDA/categorical_univariate_6.png)

![Heavy Alcohol Consumption - Univariate Distribution](figures/univariate-EDA/categorical_univariate_7.png)

![Any Healthcare - Univariate Distribution](figures/univariate-EDA/categorical_univariate_8.png)

![No Doctor Cause of Cost - Univariate Distribution](figures/univariate-EDA/categorical_univariate_9.png)

![Difficulty Walking Up Stairs - Univariate Distribution](figures/univariate-EDA/categorical_univariate_11.png)

![Smoker - Univariate Distribution](figures/univariate-EDA/categorical_univariate_14.png)

![Sex - Univariate Distribution](figures/univariate-EDA/categorical_univariate_15.png)

![Mental Health - Univariate Distribution](figures/univariate-EDA/numerical_univariate_2.png)

![Physical Health - Univariate Distribution](figures/univariate-EDA/numerical_univariate_3.png)

![Age - Univariate Distribution](figures/univariate-EDA/numerical_univariate_4.png)

![Education - Univariate Distribution](figures/univariate-EDA/numerical_univariate_5.png)

![Income - Univariate Distribution](figures/univariate-EDA/numerical_univariate_6.png)

#### Bivariate Analysis

![Fruits by Vegetables - Univariate Distribution](figures/bivariate-EDA/bivariate-2.png)

![General Health by Physical Activity - Univariate Distribution](figures/bivariate-EDA/bivariate-9.png)

### Appendix II: Tuning Models Selections {#sec-tuning}

I have provided below, specifics on how I tuned each model, stating the
parameters, as well as their associated ranges and grid levels. 

Naive Bayes: I did not tune any of the parameters for this model as I was 
using it as my baseline model thus I felt no need to execute or change the 
given parameters, thus rather just fitting the base values of it to my 
resamples. 

Logistic Regression: There were no hyperparameters for this model, thus I 
did not need to tune anything when fitting the kitchen sink version or the feature
engineering version of the model to my resamples. 

Random Forest: For my kitchen sink random forest model, I decided to tune the 
`trees`, `min_n`, and `mtry` parameters. The `trees` parameter represents the 
number of trees to be grown in the random forest. For this hyperparameter I 
decided to look at a range from 100 to 2000, with increments of 6. Thus I will 
be looking at around 317 different values for this parameter, with a grid size 
of 6 levels. The `min_n` parameter represents the minimum number of samples 
required to split a node in the tree. I decided for this parameter to go with 
the range that they already previous associated with this parameter. However,
I decided to look at the range with increments of 4. Thus I will be looking at 
around 10 different values for this parameter, with a grid size of 4 levels. 
Lastly, I decided to tune the `mtry` parameters which represents the number of 
variables randomly sampled as candidates at each split. I decided to look at 
a range from 1 to 7, in accordance with the amount of predictor variables I had, 
with increments of 1. Thus I will be looking at 7 different
values, with a grid size of 1 level. For my feature engineering random
forest model, I decided to base what I tune on how well my kitchen sink random
forest model performed beforehand. Thus for this model, I did not tune the `trees`
parameter value keeping it rather at 1000, while still tuning the `min_n` and
`mtry` parameters. For the `mtry` parameter, 
I decided to look at the range from 1 to 6, with increment of 
6. The `mtry` parameters reflects on the square root of the amount of 
predictors I had. Thus I will be looking at around 6 different values for 
this parameters, with a grid size of 6 levels. I decided to tune the `min_n`
parameters in the same manner that I did for the kitchen sink version. 

Boosted Trees: For my kitchen sink boosted tree model, I decided to tune the 
`mtry`, `min_n`, `learn_rate`, and `trees` parameters. The `trees` parameter
represents the number of trees to build in my boosted trees. For this 
hyperparameter, I decided to look at a range from 100 to 2000, with increments
of 6. Thus, I will be looking at around 317 different values for this 
parameter, with a grid size of 6 levels. The `min_n` parameter
represents the minimum number of samples required to split a node in the tree.
I decided for this parameter to go with the range that they already previous 
associated with this parameter. However, I decided to look at the range with 
increments of 4. Thus I will be looking at around 10 different values for 
this parameter, with a grid size of 4 levels. The `mtry` parameter represents 
the number of variables randomly sampled as candidates at each split. 
I decided to look at a range from 1 to 7, in accordance with the amount of 
predictor variables I had, with increments of 1. Thus I will be looking at 7 different
values, with a grid size of 1 level. Lastly, I tuned the `learn_rate` parameter, 
which controls the step size at each iteration while moving towards a minimum
of the loss function. I decided to look at a range from -5 to -0.2, with increments 
of 10. Thus, I will be looking at around 0.5 different value for this parameter,
with a grid size of 10 levels. For my feature engineering boosted tree
model, I decided to base what I tune on how well my kitchen sink 
boosted tree model performed beforehand. I did not tune the `trees`
parameter value keeping it rather at null, while still tuning the `min_n`,
`mtry`, and `learn_rate` parameters.  For the `mtry` parameter, 
I decided to look at the range from 1 to 6, with increment of 
6. The `mtry` parameters reflects on the square root of the amount of 
predictors I had. Thus I will be looking at around 6 different values for 
this parameters, with a grid size of 6 levels. I decided to tune the `min_n`
parameters in the same manner that I did for the kitchen sink version. Similarly,
for the `learn_rate` parameter I decided to tune the paramater in the same
manner that I did for the kitchen sink version, however decreasing the amount
of levels in order to be able to observe more values during the tuning 
process. Thus now looking at at grid size of 5 levels. 

$K$-Nearest Neighbors: For my kitchen sink $k$-nearest neighbors model, I 
decided to tune the `neighbors` parameter. The `neighbors` parameter represents 
the number of nearest neighbors to consider in the model. 
For this hyperparameter, I decided to look at the range from 1 to 15, 
with increments of 5. Thus I will be looking at around 3 different values 
for this parameter, with a grid size of 5 levels. Additionally, for my feature
engineering $k$-nearest neighbors model, I decided to stick with the original
range given which is 1 to 10 with increments of 5. Thus I will be looking at 
around 2 values for this parameter, with a grid size of 5 levels. 
I chose to make this change based on the selected best tuning parameter values 
for `neighbors` in the kitchen sink version. 

Neural Network: For my kitchen sink neural network model, I decided to tune the
`epochs` and `hidden_units` parameters. The `epochs` parameter represents the 
number of times the entire training dataset is passed forward and backward through the 
neural network. For this hyperparameter, I decided to stick with the original
range given which is from 10 to 1000, with increments of 5. 
Thus I will be looking at around 199 different values for this parameter, 
with a grid size of 5 levels. The `hidden_units` parameter specifies the number of neurons in
each hidden layer of the MLP. I decided for this parameter 
to also stick with the original range given which is from 1 to 10, with 
increments of 5. Therefore, I will be looking at around 2 different values for 
this parameter, with a grid size of 5 levels. For the feature engineering 
neural network model, I decided to stick with a similar tuning and resampling
method, however changing the amount of levels from 5 to 4 to ensure a faster 
model tuning sequence. 
